{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Version Information</h3><table><tr><th>Qiskit Software</th><th>Version</th></tr><tr><td>Qiskit</td><td>0.23.4</td></tr><tr><td>Terra</td><td>0.16.3</td></tr><tr><td>Aer</td><td>0.7.3</td></tr><tr><td>Ignis</td><td>0.5.1</td></tr><tr><td>Aqua</td><td>0.8.1</td></tr><tr><td>IBM Q Provider</td><td>0.11.1</td></tr><tr><th>System information</th></tr><tr><td>Python</td><td>3.7.6 | packaged by conda-forge | (default, Jun  1 2020, 18:57:50) \n",
       "[GCC 7.5.0]</td></tr><tr><td>OS</td><td>Linux</td></tr><tr><td>CPUs</td><td>2</td></tr><tr><td>Memory (Gb)</td><td>25.461994171142578</td></tr><tr><td colspan='2'>Sun May 30 04:30:22 2021 UTC</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import qiskit.tools.jupyter\n",
    "%qiskit_version_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import qiskit\n",
    "from qiskit import *\n",
    "from qiskit.compiler import assemble\n",
    "from qiskit.visualization import plot_histogram\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "shots = 4000\n",
    "number = 2\n",
    "equality = shots / np.power(2, number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept\n",
    "\n",
    "The third trial builds upon second trial. From second trial, we learned that we can't use a generative model as gradient is not differentiable through `qiskit.Circuit` (at least for now, or perhaps with my skills insufficient to make it differentiable). \n",
    "\n",
    "Then, let's subject to supervised learning. \n",
    "\n",
    "First, we will generate a dataset using random number generator substitute into the value, and then we will calculate the counts, which will be stored as probability in an arranged method, as training feature columns. \n",
    "\n",
    "Then, the parameters itself, stored in a 2D matrix, will be the target column. This will also be what we want to output from our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caveats\n",
    "\n",
    "- Perhaps there are unfeasible for larger values, as this is not the problem really about generating a giant matrix (although slow, but with current GPU and TPU is still feasible) but the number of columns scales as $\\mathcal{O}(2^n)$, where $n$ is the number of qubits. \n",
    "\n",
    "- Second is the number of data we need to generate also scales as now there are more columns, we need more data. Hence, large data means exponentially slower training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 381/2048 [00:06<00:26, 62.07it/s]/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py:7116: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "100%|██████████| 2048/2048 [00:35<00:00, 57.97it/s]\n"
     ]
    }
   ],
   "source": [
    "Dataset_size = 2048\n",
    "backend = Aer.get_backend('qasm_simulator')\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for i in tqdm(range(Dataset_size)):\n",
    "    \n",
    "    qc = QuantumCircuit(number, number)\n",
    "\n",
    "    rng = np.random.default_rng()\n",
    "    \n",
    "    random_value = []\n",
    "\n",
    "    for i in range(number):\n",
    "        inside_random = [rng.random() for i in range(3)]\n",
    "        qc.rx(inside_random[0] * np.pi, i)\n",
    "        qc.ry(inside_random[1] * np.pi, i)\n",
    "        qc.rz(inside_random[2] * np.pi, i)\n",
    "        \n",
    "        random_value = np.concatenate((random_value, inside_random))\n",
    "\n",
    "    qc.measure(range(number), range(number))\n",
    "    circ = transpile(qc, backend)\n",
    "\n",
    "    qobj = assemble(circ, shots=shots)\n",
    "\n",
    "    # Run and get counts\n",
    "    result = backend.run(qobj).result()\n",
    "    counts = result.get_counts()\n",
    "    \n",
    "    df_temp = pd.DataFrame(counts, index=[i])\n",
    "    \n",
    "    for i in range(number * 3):\n",
    "        df_temp[f\"target_{i}\"] = [random_value[i]]\n",
    "    \n",
    "    df = df.append(df_temp)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>01</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>target_0</th>\n",
       "      <th>target_1</th>\n",
       "      <th>target_2</th>\n",
       "      <th>target_3</th>\n",
       "      <th>target_4</th>\n",
       "      <th>target_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>635.0</td>\n",
       "      <td>567.0</td>\n",
       "      <td>1448.0</td>\n",
       "      <td>1350.0</td>\n",
       "      <td>0.599199</td>\n",
       "      <td>0.574853</td>\n",
       "      <td>0.899988</td>\n",
       "      <td>0.633776</td>\n",
       "      <td>0.074369</td>\n",
       "      <td>0.852806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1087.0</td>\n",
       "      <td>996.0</td>\n",
       "      <td>995.0</td>\n",
       "      <td>922.0</td>\n",
       "      <td>0.452963</td>\n",
       "      <td>0.451528</td>\n",
       "      <td>0.315275</td>\n",
       "      <td>0.514277</td>\n",
       "      <td>0.766101</td>\n",
       "      <td>0.785285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>788.0</td>\n",
       "      <td>768.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>1170.0</td>\n",
       "      <td>0.506768</td>\n",
       "      <td>0.886808</td>\n",
       "      <td>0.117003</td>\n",
       "      <td>0.767206</td>\n",
       "      <td>0.396921</td>\n",
       "      <td>0.830641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>506.0</td>\n",
       "      <td>2859.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>547.0</td>\n",
       "      <td>0.890668</td>\n",
       "      <td>0.218646</td>\n",
       "      <td>0.957858</td>\n",
       "      <td>0.775060</td>\n",
       "      <td>0.850493</td>\n",
       "      <td>0.042714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1298.0</td>\n",
       "      <td>961.0</td>\n",
       "      <td>984.0</td>\n",
       "      <td>757.0</td>\n",
       "      <td>0.427852</td>\n",
       "      <td>0.295128</td>\n",
       "      <td>0.764753</td>\n",
       "      <td>0.065553</td>\n",
       "      <td>0.457968</td>\n",
       "      <td>0.394173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       00      01      10      11  target_0  target_1  target_2  target_3  \\\n",
       "1   635.0   567.0  1448.0  1350.0  0.599199  0.574853  0.899988  0.633776   \n",
       "1  1087.0   996.0   995.0   922.0  0.452963  0.451528  0.315275  0.514277   \n",
       "1   788.0   768.0  1274.0  1170.0  0.506768  0.886808  0.117003  0.767206   \n",
       "1   506.0  2859.0    88.0   547.0  0.890668  0.218646  0.957858  0.775060   \n",
       "1  1298.0   961.0   984.0   757.0  0.427852  0.295128  0.764753  0.065553   \n",
       "\n",
       "   target_4  target_5  \n",
       "1  0.074369  0.852806  \n",
       "1  0.766101  0.785285  \n",
       "1  0.396921  0.830641  \n",
       "1  0.850493  0.042714  \n",
       "1  0.457968  0.394173  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copied_df = df.copy()\n",
    "\n",
    "copied_df = (copied_df - copied_df.min()) / (copied_df.max() - copied_df.min())\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "target = copied_df.iloc[:128, 4:]\n",
    "df = copied_df.iloc[:128, :4]\n",
    "\n",
    "# target = df.pop(\"target\")\n",
    "dataset = tf.data.Dataset.from_tensor_slices((df.values, target.values))\n",
    "\n",
    "train_dataset = dataset.cache().shuffle(len(df)).batch(batch_size)\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 128 steps\n",
      "Epoch 1/100\n",
      "128/128 [==============================] - 0s 4ms/step - loss: 0.0739\n",
      "Epoch 2/100\n",
      "128/128 [==============================] - 0s 1ms/step - loss: 0.0723\n",
      "Epoch 3/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0725\n",
      "Epoch 4/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0716\n",
      "Epoch 5/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0712\n",
      "Epoch 6/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0712\n",
      "Epoch 7/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0713\n",
      "Epoch 8/100\n",
      "128/128 [==============================] - 0s 1ms/step - loss: 0.0701\n",
      "Epoch 9/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0702\n",
      "Epoch 10/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0696\n",
      "Epoch 11/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0688\n",
      "Epoch 12/100\n",
      "128/128 [==============================] - 0s 1ms/step - loss: 0.0694\n",
      "Epoch 13/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0685\n",
      "Epoch 14/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0681\n",
      "Epoch 15/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0682\n",
      "Epoch 16/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0686\n",
      "Epoch 17/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0674\n",
      "Epoch 18/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0666\n",
      "Epoch 19/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0678\n",
      "Epoch 20/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0671\n",
      "Epoch 21/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0661\n",
      "Epoch 22/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0659\n",
      "Epoch 23/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0665\n",
      "Epoch 24/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0655\n",
      "Epoch 25/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0656\n",
      "Epoch 26/100\n",
      "128/128 [==============================] - 0s 1ms/step - loss: 0.0655\n",
      "Epoch 27/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0655\n",
      "Epoch 28/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0656\n",
      "Epoch 29/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0660\n",
      "Epoch 30/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0647\n",
      "Epoch 31/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0648\n",
      "Epoch 32/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0650\n",
      "Epoch 33/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0649\n",
      "Epoch 34/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0648\n",
      "Epoch 35/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0645\n",
      "Epoch 36/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0651\n",
      "Epoch 37/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0646\n",
      "Epoch 38/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0645\n",
      "Epoch 39/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0643\n",
      "Epoch 40/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0624\n",
      "Epoch 41/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0645\n",
      "Epoch 42/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0638\n",
      "Epoch 43/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0636\n",
      "Epoch 44/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0633\n",
      "Epoch 45/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0643\n",
      "Epoch 46/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0638\n",
      "Epoch 47/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0625\n",
      "Epoch 48/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0632\n",
      "Epoch 49/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0626\n",
      "Epoch 50/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0631\n",
      "Epoch 51/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0622\n",
      "Epoch 52/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0625\n",
      "Epoch 53/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0625\n",
      "Epoch 54/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0624\n",
      "Epoch 55/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0627\n",
      "Epoch 56/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0624\n",
      "Epoch 57/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0613\n",
      "Epoch 58/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0618\n",
      "Epoch 59/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0626\n",
      "Epoch 60/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0617\n",
      "Epoch 61/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0619\n",
      "Epoch 62/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0607\n",
      "Epoch 63/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0618\n",
      "Epoch 64/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0603\n",
      "Epoch 65/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0609\n",
      "Epoch 66/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0614\n",
      "Epoch 67/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0609\n",
      "Epoch 68/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0610\n",
      "Epoch 69/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0607\n",
      "Epoch 70/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0609\n",
      "Epoch 71/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0606\n",
      "Epoch 72/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0608\n",
      "Epoch 73/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0604\n",
      "Epoch 74/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0605\n",
      "Epoch 75/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0606\n",
      "Epoch 76/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0600\n",
      "Epoch 77/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0604\n",
      "Epoch 78/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0599\n",
      "Epoch 79/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0611\n",
      "Epoch 80/100\n",
      "128/128 [==============================] - 0s 1ms/step - loss: 0.0600\n",
      "Epoch 81/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0602\n",
      "Epoch 82/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0606\n",
      "Epoch 83/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0604\n",
      "Epoch 84/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0594\n",
      "Epoch 85/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0597\n",
      "Epoch 86/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0595\n",
      "Epoch 87/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0601\n",
      "Epoch 88/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0593\n",
      "Epoch 89/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0597\n",
      "Epoch 90/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0604\n",
      "Epoch 91/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0591\n",
      "Epoch 92/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0594\n",
      "Epoch 93/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0585\n",
      "Epoch 94/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0589\n",
      "Epoch 95/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0593\n",
      "Epoch 96/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0594\n",
      "Epoch 97/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0600\n",
      "Epoch 98/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0595\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0591\n",
      "Epoch 100/100\n",
      "128/128 [==============================] - 0s 2ms/step - loss: 0.0594\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0b3c3643d0>"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dense = tf.keras.layers.Dense\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    Dense(4, input_shape=(len(df.columns), ), activation=tf.nn.relu),\n",
    "    Dense(256, activation=tf.nn.relu),\n",
    "    Dense(6, activation=tf.nn.sigmoid)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "\n",
    "model.fit(train_dataset, epochs=100,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(np.array(pd.DataFrame([1000, 1000, 1000, 1000]).T))\n",
    "pred = np.squeeze(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qc = QuantumCircuit(number, number)\n",
    "\n",
    "for i in range(number):\n",
    "    qc.rx(pred[0 * (i + 1)] * np.pi, i)\n",
    "    qc.ry(pred[0 * (i + 2)] * np.pi, i)\n",
    "    qc.rz(pred[0 * (i + 3)] * np.pi, i)\n",
    "\n",
    "qc.measure(range(number), range(number))\n",
    "circ = transpile(qc, backend)\n",
    "\n",
    "qobj = assemble(circ, shots=shots)\n",
    "\n",
    "# Run and get counts\n",
    "result = backend.run(qobj).result()\n",
    "counts = result.get_counts()\n",
    "plot_histogram(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1., dtype=float32)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
