{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "The final method thought of without requiring to worry about non-differentiable qiskit Circuit would be to use reinforcement learning to learn. However, this would be very difficult especially most online tutorials are based on OpenAI Gym and other pre-built libraries available with the pre-built environments. And building own environment, worse still, an environment of qiskit circuit instead of games, is a nightmare. \n",
    "\n",
    "Trying our best to implement if possible. \n",
    "\n",
    "The code is takened based on: https://github.com/shivaverma/Orbit/blob/master/Paddle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Version Information</h3><table><tr><th>Qiskit Software</th><th>Version</th></tr><tr><td>Qiskit</td><td>0.27.0</td></tr><tr><td>Terra</td><td>0.17.4</td></tr><tr><td>Aer</td><td>0.8.2</td></tr><tr><td>Ignis</td><td>0.6.0</td></tr><tr><td>Aqua</td><td>0.9.2</td></tr><tr><td>IBM Q Provider</td><td>0.14.0</td></tr><tr><th>System information</th></tr><tr><td>Python</td><td>3.7.3 (default, Jan 22 2021, 20:04:44) \n",
       "[GCC 8.3.0]</td></tr><tr><td>OS</td><td>Linux</td></tr><tr><td>CPUs</td><td>4</td></tr><tr><td>Memory (Gb)</td><td>23.542434692382812</td></tr><tr><td colspan='2'>Wed Jun 23 15:51:00 2021 UTC</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import qiskit.tools.jupyter\n",
    "%qiskit_version_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qiskit\n",
    "from qiskit import *\n",
    "from qiskit.compiler import assemble\n",
    "from qiskit.visualization import plot_histogram\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import deque\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "shots = 6000\n",
    "number = 1\n",
    "\n",
    "# Equality means what are the number to get 0.5 probability for each. \n",
    "# equality = shots / np.power(2, number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 314.126x144.48 with 1 Axes>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qc = QuantumCircuit(number, number)\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "qc.rx(rng.random() * np.pi, 0)\n",
    "qc.ry(rng.random() * np.pi, 0)\n",
    "qc.rz(rng.random() * np.pi, 0)\n",
    "\n",
    "qc.measure(range(number), range(number))\n",
    "qc.draw(\"mpl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 55, '1': 5945}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backend = Aer.get_backend(\"aer_simulator\")\n",
    "circ = transpile(qc, backend)\n",
    "\n",
    "qobj = assemble(circ, shots=shots)\n",
    "\n",
    "# Run and get counts\n",
    "result = backend.run(qobj).result()\n",
    "result = result.get_counts()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 1800, '1': 4200}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = {\"0\": int(shots * 0.3), \"1\": int(shots * 0.7)}\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41130044439017516"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penalty = 0\n",
    "len_keys = len(result.keys())\n",
    "\n",
    "for i in result.keys():\n",
    "    # Root mean squared error\n",
    "    curr_penalty = np.square( (target[i] - result[i]) / shots ) / len_keys\n",
    "    penalty += np.sqrt(curr_penalty)\n",
    "\n",
    "penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41130044439017516"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(penalty, 2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning function starting below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qiskit_env():\n",
    "    \n",
    "    def __init__(self, target=target, tolerance=0.05):\n",
    "        \"\"\"\n",
    "        :var target: (dict) What do you want the result of the qiskit circuit to be? \n",
    "        :var tolerance: (float) What is the root mean squared error that you can tolerate? \n",
    "        \"\"\"\n",
    "        self.done = False\n",
    "        self.reward = 10  # To prevent extreme punishment in the beginning (only). This can be tuned. \n",
    "        self.hit, self.miss = 0, 0\n",
    "        self.backend = Aer.get_backend('aer_simulator')\n",
    "        \n",
    "        # Setup Qiskit Circuit\n",
    "        self.previous = shots * 2  # some random large number. \n",
    "        self.qc = QuantumCircuit(number, number)\n",
    "        self.target = target\n",
    "        \n",
    "        self.value = rng.random()\n",
    "        self.rank = 0\n",
    "        \n",
    "        self.counts = 0\n",
    "        self.penalty = 0\n",
    "        self.tolerance = tolerance\n",
    "        \n",
    "    def increase_val(self):\n",
    "        \"\"\"\n",
    "        Ability to increment current gate angle value.\n",
    "        \"\"\"\n",
    "        x = self.value\n",
    "        \n",
    "        # Usually will want 2 * np.pi - 0.01 * np.pi but just to make me not confuse. \n",
    "        # Plus it would not lead to invalid value, just overshoot and repeat what\n",
    "        # range we have. So we can savely proceed. \n",
    "        \n",
    "        if x < 2 * np.pi:\n",
    "            self.value += 0.01\n",
    "            \n",
    "            \n",
    "    def decrease_val(self):\n",
    "        \"\"\"\n",
    "        Ability to decrement current gate angle value.\n",
    "        \"\"\"\n",
    "        x = self.value\n",
    "        \n",
    "        # Same explanation as before. \n",
    "        \n",
    "        if x > 0:\n",
    "            self.value -= 0.01\n",
    "\n",
    "    def increase_rank(self):\n",
    "        \"\"\"\n",
    "        Ability to move horizontally between Rx, Ry and Rz. \n",
    "        \"\"\"\n",
    "        self.rank = (self.rank + 1) % 3\n",
    "\n",
    "    def decrease_rank(self):\n",
    "        \"\"\"\n",
    "        Ability to move horizontally to the left between Rx, Ry and Rz gate.\n",
    "        \"\"\"\n",
    "        self.rank = (self.rank - 1) % 3\n",
    "            \n",
    "            \n",
    "    def run_frame(self):\n",
    "        self.penalty = 0\n",
    "        \n",
    "        self.qc = QuantumCircuit(number, number)\n",
    "\n",
    "        self.qc.rx(self.value * np.pi, 0)\n",
    "        self.qc.ry(self.value * np.pi, 0)\n",
    "        self.qc.rz(self.value * np.pi, 0)\n",
    "            \n",
    "        self.qc.measure(range(number), range(number))\n",
    "        \n",
    "        circ = transpile(self.qc, self.backend)\n",
    "\n",
    "        qobj = assemble(circ, shots=shots)\n",
    "\n",
    "        # Run and get counts\n",
    "        result = self.backend.run(qobj).result()\n",
    "        self.counts = result.get_counts()\n",
    "\n",
    "        for i in self.counts.keys():\n",
    "            # Root mean squared error\n",
    "            curr_penalty = np.square( (self.target[i] - self.counts[i]) / shots ) / len(self.counts.keys())\n",
    "            self.penalty += np.sqrt(curr_penalty)\n",
    "        \n",
    "        # Doing bad job\n",
    "        if self.penalty > self.previous:\n",
    "            self.reward -= min(abs(self.penalty - self.previous), 2.5)\n",
    "            \n",
    "        # Doing good job\n",
    "        if self.penalty <= self.previous:\n",
    "            self.reward += min(abs(self.penalty - self.previous), 2.5)      \n",
    "            \n",
    "        # End of story\n",
    "        if (self.penalty) <= self.tolerance:\n",
    "            self.done = True\n",
    "            \n",
    "        self.previous = self.penalty\n",
    "            \n",
    "    def reset(self):\n",
    "        return [self.value]\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.reward = 0\n",
    "        self.done = 0\n",
    "        \n",
    "        if action == 0:\n",
    "            self.increase_val()\n",
    "            self.reward -= 0.01\n",
    "            \n",
    "        if action == 2:\n",
    "            self.decrease_val()\n",
    "            self.reward -= 0.01\n",
    "\n",
    "        if action == 3:\n",
    "            self.increase_rank()\n",
    "            self.reward -= 0.01\n",
    "\n",
    "        if action == 4:\n",
    "            self.decrease_rank()\n",
    "            self.reward -= 0.01\n",
    "            \n",
    "        self.run_frame()\n",
    "        \n",
    "        state = [self.value]\n",
    "        return self.reward, state, self.done\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Qiskit_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    \"\"\"Implementation of Deep Q learning algorithm. \"\"\"\n",
    "    \n",
    "    def __init__(self, action_space, state_space):\n",
    "        \n",
    "        self.action_space = action_space\n",
    "        self.state_space = state_space\n",
    "        self.epsilon = 1\n",
    "        self.gamma = 0.95\n",
    "        self.batch_size = 32  # Not sure how this will affect training. \n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.memory = deque(maxlen=10000)  # Increase if you have larger RAM.\n",
    "        self.model = self.build_model()\n",
    "        \n",
    "        \n",
    "    def build_model(self):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(layers.Dense(64, input_shape=(self.state_space,), activation=\"relu\"))\n",
    "        model.add(layers.Dense(64, activation=\"relu\"))\n",
    "        model.add(layers.Dense(self.action_space, activation=\"linear\"))\n",
    "        \n",
    "        model.compile(loss=\"mse\", optimizer=tf.optimizers.Adam(learning_rate=self.learning_rate))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_space)\n",
    "        \n",
    "        act_values = self.model.predict(state)\n",
    "        \n",
    "        return np.argmax(act_values[0])\n",
    "    \n",
    "    def replay(self):\n",
    "\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        states = np.array([i[0] for i in minibatch])\n",
    "        actions = np.array([i[1] for i in minibatch])\n",
    "        rewards = np.array([i[2] for i in minibatch])\n",
    "        next_states = np.array([i[3] for i in minibatch])\n",
    "        dones = np.array([i[4] for i in minibatch])\n",
    "\n",
    "        states = np.squeeze(states)\n",
    "        next_states = np.squeeze(next_states)\n",
    "\n",
    "        targets = rewards + self.gamma*(np.amax(self.model.predict_on_batch(next_states), axis=1))*(1-dones)\n",
    "        targets_full = self.model.predict_on_batch(states)\n",
    "\n",
    "        ind = np.array([i for i in range(self.batch_size)])\n",
    "#         print(targets_full[[ind], [actions]])\n",
    "        targets_new = np.empty((self.batch_size, self.action_space))\n",
    "        targets_new[[ind], [actions]] = targets\n",
    "\n",
    "        self.model.fit(states, targets_new, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not implement Action 4 and 5 yet as we still want to find out how to let the model take 2 action at a time, if required. So it not only learns to increase and decrease the rank and value at the same time instead of able to do one thing at once only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(episode):\n",
    "    loss = []\n",
    "    \n",
    "    action_space = 3  # Action space 4, 5 not yet implemented due to compatibility issue. \n",
    "    state_space = 1\n",
    "    max_steps = 1000\n",
    "    \n",
    "    agent = DQN(action_space, state_space)\n",
    "    \n",
    "    for e in range(episode):\n",
    "        state = env.reset()\n",
    "        \n",
    "        state = np.reshape(state, (1, state_space))\n",
    "        score = 0\n",
    "        \n",
    "        for i in tqdm(range(max_steps)):\n",
    "            action = agent.act(state)\n",
    "            reward, next_state, done = env.step(action)\n",
    "            \n",
    "            score += reward\n",
    "            next_state = np.reshape(next_state, (1, state_space))\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            agent.replay()\n",
    "            \n",
    "            if done:\n",
    "                print(\"episode: {}/{}, score: {}\".format(e, episode, score))\n",
    "                break\n",
    "                \n",
    "        loss.append(score)\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 266/1000 [00:23<01:10, 10.36it/s]"
     ]
    }
   ],
   "source": [
    "ep = 500\n",
    "loss = train_dqn(ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.plot([i for i in range(ep)], loss)\n",
    "plt.xlabel('episodes')\n",
    "plt.ylabel('reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
